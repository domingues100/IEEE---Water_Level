{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domingues100/IEEE---Water_Level/blob/main/SN2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORTS**"
      ],
      "metadata": {
        "id": "KBBVvw76657a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMSWRipN6v3-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from skimage import io\n",
        "from torchvision import transforms, models\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import random\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "import torch.hub\n",
        "from itertools import product, combinations\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gc\n",
        "import shutil\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import json\n",
        "import statistics"
      ],
      "metadata": {
        "id": "fkacyuTT65hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PARAMETERS**"
      ],
      "metadata": {
        "id": "pte08nJV67-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 16\n",
        "learning_rate = 0.001\n",
        "num_epochs = 250\n",
        "validation_ratio = 0.2\n",
        "margin = 1.4\n",
        "random.seed(42)\n",
        "\n",
        "\n",
        "input_image = #images path\n",
        "input_csv = #csv path\n",
        "\n",
        "output_folder = #desired output folder for pairs\n",
        "save_path =  #saving path\n",
        "\n",
        "\n",
        "train_dir = output_folder\n",
        "test_dir = train_dir"
      ],
      "metadata": {
        "id": "dqeymxPC6-HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CREATE PAIRS**"
      ],
      "metadata": {
        "id": "f3KWpbdk7V0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_pairs(df, digit_indices, num_classes):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n",
        "    for d in range(num_classes):\n",
        "        for i in range(n):\n",
        "\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
        "            pairs += [[df['pic'][z1], df['pic'][z2]]]\n",
        "            labels += [0]\n",
        "\n",
        "            inc = random.randrange(1, num_classes)\n",
        "            dn = (d + inc) % num_classes\n",
        "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
        "            pairs += [[df['pic'][z1], df['pic'][z2]]]\n",
        "            labels += [1]\n",
        "    return np.array(pairs), np.array(labels)\n",
        "\n",
        "\n",
        "def copiar_imagens(origem, destino):\n",
        "\n",
        "  if os.path.exists(output_folder):\n",
        "    shutil.rmtree(output_folder)\n",
        "\n",
        "  os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "  arquivos = os.listdir(origem)\n",
        "\n",
        "  for arquivo in arquivos:\n",
        "      if arquivo.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "          caminho_origem = os.path.join(origem, arquivo)\n",
        "          caminho_destino = os.path.join(destino, arquivo)\n",
        "\n",
        "          shutil.copyfile(caminho_origem, caminho_destino)\n",
        "\n",
        "\n",
        "def create_images_pairs():\n",
        "  copiar_imagens(input_image, output_folder)\n",
        "\n",
        "  df = pd.read_csv(input_csv)\n",
        "  df = df.rename(columns={'id': 'pic'})\n",
        "  df = df.rename(columns={'label': 'Bars_above'})\n",
        "  train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['Bars_above'])\n",
        "  train_df[\"Bars_above\"] = train_df[\"Bars_above\"].apply(lambda x: x - 3)\n",
        "  test_df[\"Bars_above\"] = test_df[\"Bars_above\"].apply(lambda x: x - 3)\n",
        "  train_df.to_csv(save_path + \"train_df\")\n",
        "  test_df.to_csv(save_path + \"teste_df\")\n",
        "\n",
        "  num_classes = 5\n",
        "\n",
        "  train_digit_indices = [train_df.index[train_df['Bars_above'] == i].tolist() for i in range(num_classes)]\n",
        "  test_digit_indices = [test_df.index[test_df['Bars_above'] == i].tolist() for i in range(num_classes)]\n",
        "\n",
        "  train_pairs, train_labels = create_pairs(train_df, train_digit_indices, num_classes)\n",
        "  test_pairs, test_labels = create_pairs(test_df, test_digit_indices, num_classes)\n",
        "\n",
        "  df_to_save = pd.DataFrame({'Grupo 1': train_pairs[:, 0], 'Grupo 2': train_pairs[:, 1], 'Match': train_labels})\n",
        "  df_to_save_teste = pd.DataFrame({'Grupo 1': test_pairs[:, 0], 'Grupo 2': test_pairs[:, 1], 'Match': test_labels})\n",
        "\n",
        "  df_to_save.to_csv(output_folder + 'formated_train.csv', index=False)\n",
        "  df_to_save_teste.to_csv(output_folder +'formated_test.csv', index=False)"
      ],
      "metadata": {
        "id": "BwCmBO8A7Xy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NETWORK TRAINING**"
      ],
      "metadata": {
        "id": "smnz7ST87xCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SiameseDataset(Dataset):\n",
        "    def __init__(self, training_csv=None, training_dir=None, transform=None):\n",
        "        self.train_df = pd.read_csv(training_csv)\n",
        "        self.train_dir = training_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image1_path = os.path.join(self.train_dir, str(self.train_df.iloc[index, 0]))\n",
        "        image2_path = os.path.join(self.train_dir, str(self.train_df.iloc[index, 1]))\n",
        "\n",
        "        img0 = Image.open(image1_path).convert('RGB')\n",
        "        img1 = Image.open(image2_path).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img0 = self.transform(img0)\n",
        "            img1 = self.transform(img1)\n",
        "\n",
        "        img0 = img0\n",
        "        img1 = img1\n",
        "\n",
        "        label = torch.tensor(int(self.train_df.iloc[index, 2]))\n",
        "\n",
        "        return img0, img1, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_df)\n",
        "\n",
        "class BaseNetwork(nn.Module):\n",
        "    def __init__(self, dropout_rate):\n",
        "        super(BaseNetwork, self).__init__()\n",
        "\n",
        "        self.resnet = models.resnet50(pretrained=True)\n",
        "\n",
        "        for param in self.resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        num_features_in = self.resnet.fc.in_features\n",
        "\n",
        "        self.resnet.fc = nn.Sequential(\n",
        "            nn.Linear(num_features_in, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(p=dropout_rate),\n",
        "            nn.Linear(256, 128))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self, base_network):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.base_network = base_network\n",
        "\n",
        "    def forward_one(self, x):\n",
        "        return self.base_network(x)\n",
        "\n",
        "    def forward(self, input1):\n",
        "        output = self.forward_one(input1)\n",
        "        return output\n",
        "\n",
        "def contrastive_loss(out1, out2, label):\n",
        "    distance = torch.sum(torch.pow(out2-out1, 2), 1)\n",
        "\n",
        "    loss_contrastive = torch.mean((1 - label) * torch.pow(distance, 2) +\n",
        "                                  (label) * torch.pow(torch.clamp(margin - distance, min=0.0), 2))\n",
        "    return loss_contrastive\n",
        "\n",
        "def train_siamese_network(model, train_loader, val_loader):\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      siamese_network.train()\n",
        "      total_loss = 0.0\n",
        "\n",
        "      for batch_idx, (img0, img1, label) in enumerate(train_loader):\n",
        "          img0, img1, label = img0.to(device), img1.to(device), label.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          output1 = siamese_network(img0)\n",
        "          output2 = siamese_network(img1)\n",
        "\n",
        "          loss = contrastive_loss(output1, output2, label)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      avg_loss = total_loss / len(train_loader)\n",
        "      train_losses.append(avg_loss)\n",
        "\n",
        "\n",
        "      siamese_network.eval()\n",
        "      val_loss = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "          for val_img0, val_img1, val_label in val_loader:\n",
        "              val_img0, val_img1, val_label = val_img0.to(device), val_img1.to(device), val_label.to(device)\n",
        "\n",
        "              val_output1 = siamese_network(val_img0)\n",
        "              val_output2 = siamese_network(val_img1)\n",
        "\n",
        "              val_loss += contrastive_loss(val_output1, val_output2, val_label)\n",
        "\n",
        "      avg_val_loss = val_loss / len(val_loader)\n",
        "      val_losses.append(avg_val_loss)\n",
        "\n",
        "      print(f\"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")"
      ],
      "metadata": {
        "id": "peC46eT9Kj9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "#pairs path\n",
        "train_csv = f'{output_folder}+/formated_train.csv'\n",
        "test_csv = f'{output_folder}+/formated_test.csv'\n",
        "\n",
        "create_images_pairs()\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.RandomAffine(\n",
        "            degrees=0.4,\n",
        "            translate=(0.3, 0.3),\n",
        "            scale=(1 - 0.4, 1 + 0.4)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "            mean=[0.485, 0.456, 0.406],\n",
        "            std=[0.229, 0.224, 0.225])   ])\n",
        "\n",
        "#Datasets\n",
        "train_dataset = SiameseDataset(training_csv=train_csv, training_dir=train_dir, transform=transform)\n",
        "validation_size = int(validation_ratio * len(train_dataset))\n",
        "train_size = len(train_dataset) - validation_size\n",
        "train_dataset, val_dataset = random_split(train_dataset, [train_size, validation_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "test_dataset = SiameseDataset(training_csv=test_csv, training_dir=test_dir,transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#Networks\n",
        "base_network = BaseNetwork(dropout_rate=0.2)\n",
        "\n",
        "siamese_network = SiameseNetwork(base_network)\n",
        "\n",
        "optimizer = optim.SGD(siamese_network.parameters(), lr=learning_rate, momentum = 0.9)\n",
        "\n",
        "#Training Step\n",
        "train_siamese_network(siamese_network.to(device), train_loader, val_loader)\n",
        "\n",
        "torch.save(siamese_network.state_dict(), f\"{save_path}/modelo1.pth\")"
      ],
      "metadata": {
        "id": "OgGJoP6_8DOa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "remember to train 5 times and save five weights/models with different names\n",
        "\n",
        "Every time there is a \"train_df\" and a \"test_df\" that are saved in save_path. You need to change there names before running again to avoid losing test and train csv. It will be used in TEST STEP"
      ],
      "metadata": {
        "id": "U3cV3Q7wp7Wc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST STEP"
      ],
      "metadata": {
        "id": "eUHl4ot34FRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "base_network = BaseNetwork(dropout_rate=0.2)\n",
        "siamese_network = SiameseNetwork(base_network)\n",
        "siamese_network .load_state_dict(torch.load(f\"{save_path}/modelo1.pth\"))\n",
        "siamese_network.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "siamese_network.to(device)\n",
        "\n",
        "#euclidean distance\n",
        "def distancia_euclidiana(vec1, vec2):\n",
        "    return np.linalg.norm(vec1 - vec2)\n",
        "\n",
        "#preprocess images\n",
        "def load_and_preprocess_image(image_path):\n",
        "    imagem = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    preprocess = transforms.Compose([transforms.Resize((224, 224)),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(\n",
        "                                     mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225]) ])\n",
        "\n",
        "    imagem = preprocess(imagem)\n",
        "    imagem = imagem.unsqueeze(0)\n",
        "    imagem = imagem.to(device)\n",
        "    return imagem\n",
        "\n",
        "def generate_feature_vector(img_path):\n",
        "    img = load_and_preprocess_image(img_path)\n",
        "    features = siamese_network(img)\n",
        "    feature_vector = features.flatten().cpu().detach().numpy()\n",
        "    return feature_vector\n",
        "\n",
        "\n",
        "def list_features_classes(images_df, path):\n",
        "    feature_vectors = []\n",
        "    class_name = []\n",
        "\n",
        "    for index, row in images_df.iterrows():\n",
        "        image = row['pic']\n",
        "        classe = row['Bars_above']\n",
        "\n",
        "        feature_vector = generate_feature_vector(os.path.join(path, image))\n",
        "        class_name.append(classe)\n",
        "        feature_vectors.append(feature_vector)\n",
        "\n",
        "    return class_name, feature_vectors\n",
        "\n",
        "\n",
        "def create_csv(classe, feature, feature_csv_name):\n",
        "    vetores_path = f'{save_path}/{feature_csv_name}'\n",
        "    data = {'Classe': classe, 'Feature_vector': feature}\n",
        "    faces_df = pd.DataFrame(data)\n",
        "    faces_df.to_csv(vetores_path, index=False)\n",
        "\n",
        "    return vetores_path\n",
        "\n",
        "def comparar_features(new_feature_vector, features_csv):\n",
        "    df = pd.read_csv(features_csv)\n",
        "\n",
        "    df['Feature_vector'] = df['Feature_vector'].apply(lambda x: np.fromstring(x[1:-1], sep=\" \"))\n",
        "\n",
        "    label = None\n",
        "    distancia_minima = float('inf')\n",
        "\n",
        "    for index, linha in df.iterrows():\n",
        "      distancia = distancia_euclidiana(new_feature_vector, np.array(linha['Feature_vector']))\n",
        "      if distancia < distancia_minima:\n",
        "          distancia_minima = distancia\n",
        "          label = linha['Classe']\n",
        "\n",
        "    return label\n",
        "\n",
        "\n",
        "def generate_metrics(results, true_label):\n",
        "    accuracy = accuracy_score(true_label, results)\n",
        "    precision = precision_score(true_label, results, average='macro')\n",
        "    recall = recall_score(true_label, results, average='macro')\n",
        "    f1 = f1_score(true_label, results, average='macro')\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "def generate_results(teste_df, vetores):\n",
        "    results = []\n",
        "    true_label = []\n",
        "    for index, row in teste_df.iterrows():\n",
        "        image_path = os.path.join(path, row['pic'])\n",
        "        classe = row['Bars_above']\n",
        "\n",
        "        new_feature_vector = generate_feature_vector(image_path)\n",
        "        label = comparar_features(new_feature_vector, vetores)\n",
        "\n",
        "        results.append(label)\n",
        "        true_label.append(classe)\n",
        "\n",
        "    accuracy, precision, recall, f1 = generate_metrics(results, true_label)\n",
        "    return results, true_label, accuracy, precision, recall, f1\n",
        "\n",
        "def generate_random_samples(csv):\n",
        "    df = pd.read_csv(csv)\n",
        "    insert_df = df.groupby('Bars_above').apply(lambda x: x.sample(n=5))\n",
        "    return insert_df\n",
        "\n",
        "def write_txt(i, accuracy, precision, recall, f1, path):\n",
        "  if not os.path.exists(path):\n",
        "      with open(path, \"w\") as arquivo:\n",
        "          arquivo.write(f\"Iteração: {i} \\n\")\n",
        "          arquivo.write(f\"Accuracy: {accuracy} \\n\")\n",
        "          arquivo.write(f\"Precision: {precision} \\n\")\n",
        "          arquivo.write(f\"Recall: {recall} \\n\")\n",
        "          arquivo.write(f\"F1: {f1} \\n\")\n",
        "  else:\n",
        "      with open(path, \"a\") as arquivo:\n",
        "          arquivo.write(f\"Iteração: {i} \\n\")\n",
        "          arquivo.write(f\"Accuracy: {accuracy} \\n\")\n",
        "          arquivo.write(f\"Precision: {precision} \\n\")\n",
        "          arquivo.write(f\"Recall: {recall} \\n\")\n",
        "          arquivo.write(f\"F1: {f1} \\n\")"
      ],
      "metadata": {
        "id": "NwPzE0gxkAWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing model**"
      ],
      "metadata": {
        "id": "sNjOJVQeUObQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = []\n",
        "pre = []\n",
        "rec = []\n",
        "f = []\n",
        "results1 = []\n",
        "true_label1 = []\n",
        "\n",
        "path = #same images path\n",
        "save_path = #same saving path\n",
        "\n",
        "\n",
        "insert_df = pd.read_csv(save_path + \"train_df\")\n",
        "test_df = pd.read_csv(f'{save_path}/teste_df')\n",
        "\n",
        "\n",
        "classe, feature = list_features_classes(insert_df, path)\n",
        "\n",
        "#choose a name\n",
        "vetores_path = create_csv(classe, feature, feature_csv_name = \"vetores_teste2_5.csv\")\n",
        "\n",
        "results, true_label, accuracy, precision, recall, f1 = generate_results(test_df, vetores_path)\n",
        "\n",
        "\n",
        "acc.append(accuracy)\n",
        "pre.append(precision)\n",
        "rec.append(recall)\n",
        "f.append(f1)\n",
        "results1.append(results)\n",
        "true_label1.append(true_label)\n",
        "\n",
        "df = pd.DataFrame({'results': results1, 'true_label': true_label1})\n",
        "df.to_csv(f'{save_path}/SN2/df1.csv', index=False)\n",
        "#remember to change df1 in each training step that you made and change train_df/test_df/model name for each training"
      ],
      "metadata": {
        "id": "04n_0-F1JGiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = []\n",
        "pre = []\n",
        "rec = []\n",
        "f = []\n",
        "results1 = []\n",
        "true_label1 = []\n",
        "\n",
        "path = #same images path\n",
        "save_path = #same saving path\n",
        "\n",
        "#this will run 50 times. 3 TESTS were made, you need to run this for each test. FOR 1 sample, 3 samples or 5 samples. And you need to\n",
        "#repeat the process for each model that was trained. REMEMBER TO CHANGE IN generate_random_samples() the x.sample(n=5), this will produce 5 samples.\n",
        "#you need to change for 1, 3 or 5. And do it again for each model.\n",
        "\n",
        "\n",
        "for i in range(50):\n",
        "  insert_df = generate_random_samples(f'{save_path}/train_df')\n",
        "  test_df = pd.read_csv(f'{save_path}/teste_df')\n",
        "\n",
        "  #create features csv\n",
        "  classe, feature = list_features_classes(insert_df, path)\n",
        "\n",
        "  #choose the name that you want\n",
        "  vetores_path = create_csv(classe, feature, feature_csv_name = \"vetores_teste2_1.csv\")\n",
        "\n",
        "  #gerar os resultados\n",
        "  results, true_label, accuracy, precision, recall, f1 = generate_results(test_df, vetores_path)\n",
        "\n",
        "\n",
        "  acc.append(accuracy)\n",
        "  pre.append(precision)\n",
        "  rec.append(recall)\n",
        "  f.append(f1)\n",
        "  results1.append(results)\n",
        "  true_label1.append(true_label)\n",
        "\n",
        "  write_txt(i, accuracy, precision, recall, f1, (save_path+\"/teste5_1.txt\"))\n",
        "\n",
        "df = pd.DataFrame({'results': results1, 'true_label': true_label1})\n",
        "df.to_csv(f'{save_path}/SN2/df5_{i}.csv', index=False)\n",
        "\n",
        "#here you need to change the test5_1.txt for the name that you want.\n",
        "write_txt(\"média\", sum(acc)/len(acc), sum(pre)/len(pre), sum(rec)/len(rec), sum(f)/len(f), (save_path+\"/teste5_1.txt\"))"
      ],
      "metadata": {
        "id": "FgfK9WRKJHu0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}